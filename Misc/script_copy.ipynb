{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "from langchain.prompts import PromptTemplate\n",
    "from tenacity import retry, wait_random_exponential\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load csv file\n",
    "data = pd.read_json(\n",
    "    path_or_buf=\"response_curation_prompt_set_08012023_67802 prompts.jsonl\", lines=True\n",
    ")\n",
    "\n",
    "# test first 8 prompts end to end\n",
    "# data = data[:8]\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List targets domains\n",
    "domains = [\n",
    "    \"science\",\n",
    "    \"technology\",\n",
    "    \"engineering\",\n",
    "    \"math\",\n",
    "    \"health\",\n",
    "    \"coding\",\n",
    "    \"business\",\n",
    "    \"general\",\n",
    "    \"finance\",\n",
    "    \"legal\",\n",
    "    \"writing\",\n",
    "]\n",
    "\n",
    "# Get prompt data\n",
    "prompt_column = \"prompt\"\n",
    "prompt_data = data[prompt_column]\n",
    "\n",
    "\n",
    "# Batch data\n",
    "batch_size = 8  # default batch size\n",
    "prompt_data_batches = [\n",
    "    prompt_data.iloc[i : i + batch_size] for i in range(0, len(prompt_data), batch_size)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String representation of domains\n",
    "def str_domains(domains):\n",
    "    result = \"\"\n",
    "    for i in range(len(domains)):\n",
    "        if i < len(domains) - 1:\n",
    "            result += f\"{domains[i]}, \"\n",
    "        else:\n",
    "            result += f\"and {domains[i]}\"\n",
    "    return result\n",
    "\n",
    "\n",
    "# Zero Shot Template\n",
    "Zero_Shot_template = \"\"\"You are a domain classification expert.\n",
    "Your task is to analyse a given prompt and classify it into one of {nb_domains} domains.\n",
    "The seven domains to choose from are {str_domains}.\n",
    "                      \n",
    "Prompt:\n",
    "                      \n",
    "{prompt}\n",
    "                      \n",
    "The domain classification is:\"\"\"\n",
    "\n",
    "Zero_Shot = PromptTemplate(\n",
    "    template=Zero_Shot_template,\n",
    "    input_variables=[\"prompt\"],\n",
    "    partial_variables={\"nb_domains\": len(domains), \"str_domains\": str_domains(domains)},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt formatter\n",
    "def format_prompts(prompts, prompt_template):\n",
    "    formatted_prompts = []\n",
    "    for p in prompts:\n",
    "        prompt_formatted = prompt_template.format(prompt=p)\n",
    "        formatted_prompts.append(prompt_formatted)\n",
    "    return formatted_prompts\n",
    "\n",
    "\n",
    "# Cleaning and formatting of AI responses; commented code is for logprobs if not 0\n",
    "def extract_domains(responses):\n",
    "    # top_logprobs = [response['logprobs']['top_logprobs'][0].keys() for response in responses.choices]\n",
    "    extracted = [response[\"text\"] for response in responses.choices]\n",
    "\n",
    "    # def is_domain(d):\n",
    "    #     return d in domains\n",
    "    def clean_resp(resp):\n",
    "        resp = re.sub(r\"[^a-zA-Z ]\", \"\", resp)\n",
    "        resp = resp.lower().strip()\n",
    "        return resp\n",
    "\n",
    "    cleaned_choices = [clean_resp(choice) for choice in extracted]\n",
    "    # top_domains = (filter(is_domain, cleaned_choices)) # only take choices that match domains\n",
    "    # top_domains = list(dict.fromkeys(top_domains)) # remove duplicates\n",
    "    # top_domains = ', '.join(top_domains)\n",
    "    # results.append(top_domains)\n",
    "    # return results\n",
    "    return cleaned_choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get API responses\n",
    "openai.api_key = \"api_key\"\n",
    "\n",
    "\n",
    "@retry(\n",
    "    wait=wait_random_exponential(min=5, max=60)\n",
    ")  # necessary retry wrapper to handle Rate Limit Error\n",
    "def get_openai_response(prompts, answers_per_resp=0):\n",
    "    responses = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompts,\n",
    "        max_tokens=2,\n",
    "        logprobs=answers_per_resp,  # get top <=answer_per_resp predictions\n",
    "    )\n",
    "    return extract_domains(responses)\n",
    "\n",
    "\n",
    "# I suggest uncommenting the count variable and its printing because it allows you to see if there is anything wrong with the api calls\n",
    "def get_domain_predictions(prompt_template):\n",
    "    predictions = []\n",
    "    failures = []\n",
    "\n",
    "    for batch in tqdm(prompt_data_batches):\n",
    "        try:\n",
    "            formatted_prompts = format_prompts(list(batch), prompt_template)\n",
    "            predictions.extend(get_openai_response(formatted_prompts))\n",
    "\n",
    "        except:\n",
    "            failures.append(batch)\n",
    "\n",
    "    return predictions, failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_predictions, failures = get_domain_predictions(Zero_Shot)\n",
    "print(len(domain_predictions), len(failures))\n",
    "\n",
    "data[\"domain_classifications\"] = domain_predictions\n",
    "data.to_csv(\n",
    "    \"domain_classified_data.csv\", index=False\n",
    ")  # output CSV file of slice of data + corresponding predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
